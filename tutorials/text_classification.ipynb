{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL60gfOrcILF"
      },
      "source": [
        "%%capture\n",
        "!pip install https://3388-217161669-gh.circle-artifacts.com/0/wheels/torch-1.7.0a0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install https://3388-217161669-gh.circle-artifacts.com/0/wheels/nestedtensor-0.0.1.dev20201173-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s1tLazJct6y"
      },
      "source": [
        "import re\n",
        "import requests\n",
        "import io\n",
        "import tarfile\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import sys\n",
        "import concurrent.futures\n",
        "import time\n",
        "from collections import Counter\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import nestedtensor\n",
        "\n",
        "URL = \"https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg3h4hOJcILJ"
      },
      "source": [
        "Point = namedtuple('Point', 'label text')\n",
        "\n",
        "def get_data(URL):\n",
        "    r = requests.get(URL)\n",
        "    file_like_object = io.BytesIO(r.content)\n",
        "    tar = tarfile.open(fileobj=file_like_object)\n",
        "    d = {}\n",
        "    for member in tar.getmembers():\n",
        "        if member.isfile() and member.name.endswith('csv'):\n",
        "            k = 'train' if 'train' in member.name else 'test'\n",
        "            d[k] = tar.extractfile(member)\n",
        "    return d\n",
        "\n",
        "\n",
        "def preprocess(iterator):\n",
        "    def _preprocess(line):\n",
        "        line = line.decode('UTF-8')\n",
        "        line = line.lower()\n",
        "        line = re.sub(r'[^0-9a-zA-Z,\\s]', \"\", line)\n",
        "        line = line.split(',')\n",
        "        label = int(line[0]) - 1\n",
        "        text = (\" \".join(line[1:])).split()\n",
        "        if len(line) > 2:\n",
        "            return Point(label=label, text=text)\n",
        "    for line in iterator:\n",
        "        yield _preprocess(line)\n",
        "\n",
        "\n",
        "def build_vocab(iterator):\n",
        "    counter = Counter()\n",
        "    labels = set()\n",
        "    for point in iterator:\n",
        "        counter.update(point.text)\n",
        "        labels.add(point.label)\n",
        "    vocab = {}\n",
        "    for i, (word, count) in enumerate(counter.most_common()):\n",
        "        vocab[word] = i\n",
        "\n",
        "    return vocab, labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_G7pmJUcILL"
      },
      "source": [
        "data = get_data(URL)\n",
        "data = {k: list(preprocess(v)) for (k, v) in data.items()}\n",
        "vocab, labels = build_vocab(data['train'])\n",
        "UNK = len(vocab)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-2xf24FcILO"
      },
      "source": [
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.fc(self.embedding(text))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fS1Eo9jcILQ"
      },
      "source": [
        "embed_dim = 10\n",
        "model = TextSentiment(len(vocab) + 1, embed_dim, len(labels))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob4HSMOMcILT"
      },
      "source": [
        "def create_batch(data):\n",
        "    data = nestedtensor.nested_tensor(\n",
        "        [torch.tensor(list(map(lambda x: vocab.get(x, UNK), tokens))) for tokens in data], dtype=torch.int64)\n",
        "    return data\n",
        "\n",
        "def yield_data(data):\n",
        "    random.shuffle(data)\n",
        "    labels = []\n",
        "    batch_data = []\n",
        "    for i, point in enumerate(data):\n",
        "        # Stop accumulating lines of text once we reach 4000 tokens or more\n",
        "        # This yields variable batch sizes, but with consistent memory pressure\n",
        "        if sum(map(len, batch_data), 0) < 4000:\n",
        "            labels.append(torch.tensor(point.label))\n",
        "            batch_data.append(point.text)\n",
        "        else:                    \n",
        "            yield (nestedtensor.nested_tensor(labels, dtype=torch.int64), create_batch(batch_data))\n",
        "            labels = []\n",
        "            batch_data = []"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBZLiRh2cILV",
        "outputId": "d2dee4bf-cecc-4e1b-8350-d0f31d3e5676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_tokens = sum(map(lambda x: len(x.text), data['train']))\n",
        "print(\"Total number of tokens: {}\".format(num_tokens))\n",
        "for epoch in range(2):\n",
        "    i = 0\n",
        "    t0 = time.time()\n",
        "    for labels, future in yield_data(data['train']):\n",
        "        batch = future\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, labels).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        sys.stderr.write(\n",
        "            \"\\rtime: {:3.0f}s epoch: {:3.0f} lr: {:3.6f} loss: {:3.6f}\".format(\n",
        "                time.time() - t0, \n",
        "                epoch, \n",
        "                scheduler.get_last_lr()[0],\n",
        "                loss, \n",
        "            )\n",
        "        )\n",
        "        sys.stderr.flush()\n",
        "        i += batch.numel()\n",
        "        if i > 1000000:\n",
        "            scheduler.step()\n",
        "            i = 0\n",
        "    sys.stderr.write('\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tokens: 27205880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "time: 372s epoch:   0 lr: 0.263520 loss: 0.244233\n",
            "time: 376s epoch:   1 lr: 0.069443 loss: 0.343512\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v05_XWkYcILY",
        "outputId": "af465179-3f49-421e-8255-bf03801abb2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_correct = 0\n",
        "total_num = 0\n",
        "for tb in yield_data(data['test']):\n",
        "  output = model(tb[1]).to_tensor().argmax(1)\n",
        "  num_correct += (tb[0].to_tensor() == output).sum().item()\n",
        "  total_num += len(output)\n",
        "\n",
        "print(\"Test accuracy: {}\".format(float(num_correct) / float(total_num)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9362489876200394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PT0PxhFcILa"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}